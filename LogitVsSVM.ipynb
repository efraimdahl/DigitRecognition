{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data reading and splitting.\n",
    "mnist_data = pd.read_csv('mnist.csv').values\n",
    "labels = mnist_data[:, 0]\n",
    "digits = mnist_data[:, 1:]\n",
    "\n",
    "#random_state ensures that the split data is the same for each program iteration.\n",
    "x_train, x_test, y_train, y_test = train_test_split(digits, labels, test_size=0.88, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Regularized multinomial logit model (using the LASSO penalty).</h3>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "The logistic regression classifier by default applies L2 (Ridge) regularisation using the lbfgs solver. The liblinear and the saga solver, support both L1 (Lasso) and L2 regularisation. Liblinear is used for one vs rest schemes, which is appropriate for digit classification, and yields an accuracy score of 100% on the training set. The saga solver performed slightley worse, with an accuracy of 99% on the training set. When we however, apply cross validation (5-fold) the sage solver outperforms the liblinear classifier with 88% mean accuracy vs 0.84%. \n",
    "\n",
    "In a first round of hyperparameter tuning we used Cross Validation Grid search to apply 5-fold cross validation to all possible combinations of chosen parameters. The first round we tested five possible values for C [0.1,0.5,1,1.5,2] with the saga solver over 25 fits. The best fit was found at C=0.5 \n",
    "\n",
    "In a second round, we added the solver as a hyperparameter with two options liblinear and saga and tested the following C values [0.3,0.4,0.5,0.6,0.7]. Where the best fit was present at 0.3 with the saga solver. (total of 50 fits) Yielding a crossvalidation accuracy of 88.6%\n",
    "\n",
    "In a third round we tested the C-values [0.1,0.15,0.2,0.25,0.3,0.35] Here C=2 yielded the best result, which was rounded to 88.65, marginally better than for C=3. So we stopped tuning. And chose parameters C=0.2 and solver=saga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitClf = LogisticRegression(random_state=0, penalty=\"l1\", solver=\"saga\").fit(x_train,y_train)\n",
    "logitPred = logitClf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitClfLib = LogisticRegression(random_state=0, penalty=\"l1\", solver=\"liblinear\").fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitClf.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitClfLib.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, logitPred, labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "disp.plot()\n",
    "plt.title(\"Logistic Regression, saga solver with L1 regularisation, digit features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>K-Fold Cross Validation for Hyperparameter Tuning</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle= True, random_state= 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_scores = cross_val_score(estimator= logitClf,  X= x_train, y= y_train, cv= skf)\n",
    "val_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores2 = cross_val_score(estimator= logitClfLib,  X= x_train, y= y_train, cv= skf)\n",
    "val_scores2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test 1: [0.1,0.5,1,1.5,2] result C:0.5 on saga solver\n",
    "#test 2 params = {'C':  [0.3,0.4,0.5,0.6,0.7],'solver':[\"saga\",'liblinear']}\n",
    "\n",
    "params = {'C':  [0.1,0.15,0.2,0.25,0.3,0.35]}\n",
    "\n",
    "best_logitComposition = GridSearchCV(estimator=logitClf, param_grid=params,\n",
    "                              cv= skf, verbose= True, n_jobs=-1)\n",
    "\n",
    "best_logitComposition.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logitComposition.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logitComposition.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SVM Hyperparameter tuning</h3>\n",
    "Initial test (pre tuning), accuracy on test-data 0.11, accuracy on training data 1. -> A lot of overfitting.\n",
    "\n",
    "5 fold cross validation\n",
    "Test 1: #Test1: params = {'C':[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,1,1.5,2]} best C=2, cv accuracy: 0.886\n",
    "\n",
    "Test 2: {'C':[1.5,1.7,1.9,2,2.2,2.4,2.5,2.8,3.0]}        best {'C': 3} cv accuracy 0.955\n",
    "\n",
    "Test 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmClf = SVC(gamma='scale').fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9495941558441559"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmClf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "#params = {'C':[0.1,0.5,1,1.5,2]}\n",
    "#Test1: params = {'C':[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,1,1.5,2]}\n",
    "params = {'C':[3.0,3.5,4.0,4.5,5,5.5]}\n",
    "\n",
    "best_svmComposition = GridSearchCV(estimator=svmClf, param_grid=params,\n",
    "                              cv= skf, verbose= True, n_jobs=-1)\n",
    "\n",
    "best_svmComposition.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 3.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svmComposition.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9557539682539682"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svmComposition.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Comparison</h3>\n",
    "\n",
    "In the previous step we have determined the best values for hyperparameter C, which for both the LogisticRegression Classifier and the SupportVector Classifier signifies the inverse regularisation strenght, using a grid parameter search over a 5 fold cross validation. Smaller values, specify stronger regularisation. For the LogisticRegression Classifier we additionally determined the best solver.  \n",
    "\n",
    "We continue with the following parameters:\n",
    "LogisticRegression: C=0.2, solver=saga \n",
    "SVC: C=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalLogitClf = LogisticRegression(random_state=0, C=0.2, penalty=\"l1\", solver=\"saga\").fit(x_train,y_train)\n",
    "finalLogitPred = logitClf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalLogitClf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, finalLogitPred, labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "disp.plot()\n",
    "plt.title(\"Logistic Regression, saga solver with L1 regularisation, C=0.2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalSvmClf = SVC(gamma='scale').fit(x_train,y_train)\n",
    "finalSvmPred = finalSvmClf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalSvmClf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, finalSvmPred, labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "disp.plot()\n",
    "plt.title(\"SVC, C=0.05\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
